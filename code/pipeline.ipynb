{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cd5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data\n",
    "# Clean/preprocess/transform the data\n",
    "# Train a machine learning model\n",
    "# Evaluate and optimise the model\n",
    "# Clean/preprocess/transform new data\n",
    "# Fit the model on new data to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636438b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,recall_score,confusion_matrix, roc_auc_score, precision_score, f1_score, roc_curve, auc, plot_confusion_matrix,plot_roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9a9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name: X_train_des\n",
      "file name: y_test_des\n",
      "file name: X_test_des\n",
      "file name: train_month_1\n",
      "file name: data_merged\n",
      "file name: train_month_2\n",
      "file name: test_month_1\n",
      "file name: test_month_3\n",
      "file name: test_month_2\n",
      "file name: y_train_des\n",
      "file name: train_month_3_with_target\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "mypath = \"../data/\"\n",
    "mydata = load_data.get_file_names(mypath)\n",
    "data_files = load_data.load_copy_data(mydata, mypath)\n",
    "\n",
    "data = data_files['data_merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct test, train set\n",
    "X = data.drop('target',axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192816dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 49408, 1: 1549})\n",
      "Counter({0: 49408, 1: 24704})\n",
      "Counter({0: 3098, 1: 1549})\n"
     ]
    }
   ],
   "source": [
    " # summarize class distribution\n",
    "print(Counter(y_train))\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "# summarize class distribution\n",
    "print(Counter(y_over))\n",
    "print(Counter(y_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64964d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process individual columns\n",
    "\n",
    "# Change dates to inbetween years\n",
    "class InbetweenDays(BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_dataset):\n",
    "        list_dates = ['customer_since_all_x', 'customer_since_bank_x', 'customer_birth_date_x']\n",
    "\n",
    "        #Convert date columns into datetime format\n",
    "        x_dataset['base_dt'] = pd.to_datetime('2018-01-01')\n",
    "        x_dataset[list_dates] = x_dataset[list_dates].apply(pd.to_datetime)\n",
    "\n",
    "        for col in list_dates:\n",
    "            x_dataset[col] = abs(x_dataset['base_dt'].dt.year - x_dataset[col].dt.year)\n",
    "\n",
    "        #Drop columns (base_dt)\n",
    "        x_dataset = x_dataset.drop('base_dt', axis=1)\n",
    "    \n",
    "        return x_dataset\n",
    "    \n",
    "# Change Children Status to binary\n",
    "class ChildrenStatus(BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_dataset):\n",
    "        for column in ['customer_children', 'customer_children_y', 'customer_children_x']:\n",
    "            x_dataset[column].replace(['mature', 'young', 'onebaby', 'adolescent', 'preschool', 'grownup'], 'yes', inplace=True)\n",
    "            x_dataset[column].fillna('no', inplace=True)\n",
    "    \n",
    "        return x_dataset\n",
    "\n",
    "# Categorize Area Code by 1000s\n",
    "class AreaCode(BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_dataset):\n",
    "        # Bin area codes by 1000s \n",
    "        labels = [\"{}_area_code\".format(i) for i in range(0, 10000, 1000)]\n",
    "        x_dataset['area_cat'] = pd.cut(x_dataset['customer_postal_code_x'], range(0, 10005, 1000), right=False, labels=labels)\n",
    "        \n",
    "        return x_dataset\n",
    "\n",
    "# Get % Change from time point 1 to 2, 2 to 3, 1 to 3\n",
    "class PercetChange(BaseEstimator):\n",
    "\n",
    "    # bal_insurance_21: balance on \"tak 21\" life insurance\n",
    "    # bal_insurance_23: balance on \"tak 23\" life insurance\n",
    "    # bal_personal_loan: outstanding balance on personal loans\n",
    "    # bal_mortgage_loan: outstanding balance on mortgage loans\n",
    "    # bal_current_account: balance on current (checkings) accounts\n",
    "    # bal_pension_saving: balance on pension (retirement) savings accounts\n",
    "    # bal_savings_account: balance on savings accounts\n",
    "    # bal_current_account_starter: balance on starter current (checkings) accounts\n",
    "    # bal_savings_account_starter: balance on starter savings accounts\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_dataset):\n",
    "\n",
    "        def percentage_change(col1,col2):\n",
    "            return ((col2 - col1) / col1) * 100\n",
    "\n",
    "        list_balances = [['bal_insurance_21_x', 'bal_insurance_21_y', 'bal_insurance_21'],\n",
    "                         ['bal_insurance_23_x', 'bal_insurance_23_y', 'bal_insurance_23'],\n",
    "                         ['bal_personal_loan_x', 'bal_personal_loan_y', 'bal_personal_loan'],\n",
    "                         ['bal_mortgage_loan_x', 'bal_mortgage_loan_y', 'bal_mortgage_loan'],\n",
    "                         ['bal_current_account_x', 'bal_current_account_y', 'bal_current_account'],\n",
    "                         ['bal_pension_saving_x', 'bal_pension_saving_y', 'bal_pension_saving'],\n",
    "                         ['bal_savings_account_x', 'bal_savings_account_y', 'bal_savings_account'],\n",
    "                        ]\n",
    "\n",
    "        for balance_type in list_balances:\n",
    "            x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]]) \n",
    "            x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]]) \n",
    "            x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]]) \n",
    "\n",
    "            x_dataset['{}_1'.format(balance_type[2])] = x_dataset['{}_1'.format(balance_type[2])].fillna(0)\n",
    "            x_dataset['{}_2'.format(balance_type[2])] = x_dataset['{}_2'.format(balance_type[2])].fillna(0)\n",
    "            x_dataset['{}_3'.format(balance_type[2])] = x_dataset['{}_3'.format(balance_type[2])].fillna(0)    \n",
    "            \n",
    "        return x_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e04e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to preprocess\n",
    "\n",
    "# Features to drop\n",
    "drop_features = [\n",
    "    'Unnamed: 0',\n",
    "    'client_id', #ID not needed in the training data\n",
    "    'customer_education_x', #Remove education b/c missing > 70%\n",
    "    'customer_education_y', #\n",
    "    'customer_education',#\n",
    "]\n",
    "\n",
    "# Duplicated Columns to drop\n",
    "dup_cols = data.T.duplicated().reset_index()\n",
    "dup_cols_list = dup_cols.loc[dup_cols[0], 'index'].tolist()\n",
    "drop_features.extend(dup_cols_list)\n",
    "\n",
    "\n",
    "# Categorical Features to Preprocess\n",
    "categorical_features = [\n",
    "    'customer_relationship', 'customer_relationship_y', 'customer_relationship_x', \n",
    "    'customer_occupation_code_x',\n",
    "    'customer_children', 'customer_children_y', 'customer_children_x',\n",
    "    'customer_gender_x',\n",
    "    'area_cat',\n",
    "    'customer_self_employed', 'customer_self_employed_y', 'customer_self_employed_x',\n",
    "]\n",
    "\n",
    "# Numerical Features to Preprocess\n",
    "numeric_features = [\n",
    "    'bal_insurance_21', 'bal_insurance_21_y', 'bal_insurance_21_x', \n",
    "    'bal_insurance_23', 'bal_insurance_23_y', 'bal_insurance_23_x',\n",
    "    'cap_life_insurance_fixed_cap', 'cap_life_insurance_fixed_cap_y', 'cap_life_insurance_fixed_cap_x',\n",
    "    'cap_life_insurance_decreasing_cap', 'cap_life_insurance_decreasing_cap_y', 'cap_life_insurance_decreasing_cap_x',\n",
    "    'prem_fire_car_other_insurance', 'prem_fire_car_other_insurance_y', 'prem_fire_car_other_insurance_x',\n",
    "    'bal_personal_loan', 'bal_personal_loan_y', 'bal_personal_loan_x',\n",
    "    'bal_mortgage_loan', 'bal_mortgage_loan_y', 'bal_mortgage_loan_x',\n",
    "    'bal_current_account', 'bal_current_account_y', 'bal_current_account_x',\n",
    "    'bal_pension_saving', 'bal_pension_saving_y', 'bal_pension_saving_x', \n",
    "    'bal_savings_account', 'bal_savings_account_y', 'bal_savings_account_x',\n",
    "    'bal_current_account_starter', 'bal_current_account_starter_y', 'bal_current_account_starter_x',\n",
    "    'bal_savings_account_starter', 'bal_savings_account_starter_y', 'bal_savings_account_starter_x',\n",
    "    'visits_distinct_so', 'visits_distinct_so_y', 'visits_distinct_so_x',\n",
    "    'visits_distinct_so_areas', 'visits_distinct_so_areas_y', 'visits_distinct_so_areas_x',\n",
    "    'customer_since_all_x', 'customer_since_bank_x', 'customer_birth_date_x',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53cb65e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has_savings_account_starter_y',\n",
       " 'customer_since_all_y',\n",
       " 'customer_since_bank_y',\n",
       " 'customer_gender_y',\n",
       " 'customer_birth_date_y',\n",
       " 'customer_postal_code_y',\n",
       " 'customer_occupation_code_y',\n",
       " 'customer_education_y',\n",
       " 'customer_since_all',\n",
       " 'customer_since_bank',\n",
       " 'customer_gender',\n",
       " 'customer_birth_date',\n",
       " 'customer_postal_code',\n",
       " 'customer_occupation_code',\n",
       " 'customer_education']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Duplicated Columns\n",
    "dup_cols_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f540a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA with column mean, normalize numerical values\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('normalize', Normalizer()),\n",
    "])\n",
    "\n",
    "# Replace NA with column mode, encode categorical value to 0/1\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857e229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process pipeline that drops unnecessary features, transforms numerical and categorical values\n",
    "preprocessor = ColumnTransformer(remainder='passthrough',\n",
    "                                 transformers=[\n",
    "                                     ('drop_columns', 'drop', drop_features),\n",
    "                                     ('numeric', numeric_transformer, numeric_features),\n",
    "                                     ('categorical', categorical_transformer, categorical_features)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba064d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline from preprocessing to fitting a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('percent_change', PercetChange()),\n",
    "    ('area_code_categorize', AreaCode()),\n",
    "    ('inbetween_days', InbetweenDays()),\n",
    "    ('binarize_children', ChildrenStatus()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('catboost_weighted', CatBoostClassifier(verbose=False,random_state=0)),#,scale_pos_weight=2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53fdd673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_1'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[1]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_2'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[1]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['{}_3'.format(balance_type[2])] = percentage_change(x_dataset[balance_type[0]],x_dataset[balance_type[2]])\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['area_cat'] = pd.cut(x_dataset['customer_postal_code_x'], range(0, 10005, 1000), right=False, labels=labels)\n",
      "/var/folders/xc/tvrxkd8d2d71ksnxgn8wf9t40000gn/T/ipykernel_1567/3824788444.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  x_dataset['base_dt'] = pd.to_datetime('2018-01-01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('percent_change', PercetChange()),\n",
       "                ('area_code_categorize', AreaCode()),\n",
       "                ('inbetween_days', InbetweenDays()),\n",
       "                ('binarize_children', ChildrenStatus()),\n",
       "                ('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('drop_columns', 'drop',\n",
       "                                                  ['Unnamed: 0', 'client_id',\n",
       "                                                   'customer_education_x',\n",
       "                                                   'customer_education_y',\n",
       "                                                   'customer_education',\n",
       "                                                   'ha...\n",
       "                                                  ['customer_relationship',\n",
       "                                                   'customer_relationship_y',\n",
       "                                                   'customer_relationship_x',\n",
       "                                                   'customer_occupation_code_x',\n",
       "                                                   'customer_children',\n",
       "                                                   'customer_children_y',\n",
       "                                                   'customer_children_x',\n",
       "                                                   'customer_gender_x',\n",
       "                                                   'area_cat',\n",
       "                                                   'customer_self_employed',\n",
       "                                                   'customer_self_employed_y',\n",
       "                                                   'customer_self_employed_x'])])),\n",
       "                ('catboost_weighted',\n",
       "                 <catboost.core.CatBoostClassifier object at 0x7ff2dae14670>)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline.fit(X_train, y_train)\n",
    "pipeline.fit(X_over, y_over)\n",
    "#pipeline.fit(X_under, y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc06b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9497\n",
      "Recall: 0.1126\n",
      "ROC_AUC: 0.5435\n",
      "Precision: 0.1142\n"
     ]
    }
   ],
   "source": [
    "# With scale_pos_weight=5, minority class gets 5 times more impact and 5 times more correction than errors made on the majority class.\n",
    "#catboost_5 = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=5)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred),4)}')\n",
    "print(f'Recall: {round(recall_score(y_test, y_pred),4)}')\n",
    "print(f'ROC_AUC: {round(roc_auc_score(y_test, y_pred),4)}')\n",
    "print(f'Precision: {round(precision_score(y_test, y_pred),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3981d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.964\n",
      "Recall: 0.2\n",
      "ROC_AUC: 0.5898\n",
      "Precision: 0.1667\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {round(accuracy_score(y_test[:250], y_pred[:250]),4)}')\n",
    "print(f'Recall: {round(recall_score(y_test[:250], y_pred[:250]),4)}')\n",
    "print(f'ROC_AUC: {round(roc_auc_score(y_test[:250], y_pred[:250]),4)}')\n",
    "print(f'Precision: {round(precision_score(y_test[:250], y_pred[:250]),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef5dc409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535321821036107"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b049a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Churners       0.97      0.97      0.97     12376\n",
      "    Churners       0.11      0.11      0.11       364\n",
      "\n",
      "    accuracy                           0.95     12740\n",
      "   macro avg       0.54      0.54      0.54     12740\n",
      "weighted avg       0.95      0.95      0.95     12740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Non-Churners', 'Churners']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f911d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1323e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4794b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c6755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a LGBM Class\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "# Pipeline from preprocessing to fitting a model\n",
    "pipeline2 = Pipeline(steps=[\n",
    "    ('inbetween_days', InbetweenDays()),\n",
    "    ('binarize_children', ChildrenStatus()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('LGBMClassifier', LGBMClassifier(random_state=0))\n",
    "#     ('logistic', LogisticRegression(random_state=0)),\n",
    "])\n",
    "\n",
    "pipeline2.fit(X_over, y_over)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred2 = pipeline2.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred2),4)}')\n",
    "print(f'Recall: {round(recall_score(y_test, y_pred2),4)}')\n",
    "print(f'ROC_AUC: {round(roc_auc_score(y_test, y_pred2),4)}')\n",
    "print(f'Precision: {round(precision_score(y_test, y_pred2),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fefbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
